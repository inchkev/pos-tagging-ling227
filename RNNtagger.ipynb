{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNtagger.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuFkl0WycKM1"
      },
      "source": [
        "# Ling 227 Final Project\n",
        "# Neal Ma, Nick Schoelkopf, Kevin Chen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_ZjU4WbcTff"
      },
      "source": [
        "RNN (bidirectional LSTM) POS Tagger implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-2-MWGQceET"
      },
      "source": [
        "# English POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDeP8Vmajp0X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "627805d0-2564-4ff1-d4e9-0f88df47c701"
      },
      "source": [
        "# import necessary modules and allow GPU to be used (for faster training) if GPU available\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "! pip install pyconll\n",
        "import pyconll\n",
        "\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyconll\n",
            "  Downloading https://files.pythonhosted.org/packages/60/7f/148a5b6f99b8a22373bfbcafd9d6776278fec14810ae95c4fe37965f6619/pyconll-3.0.4-py3-none-any.whl\n",
            "Installing collected packages: pyconll\n",
            "Successfully installed pyconll-3.0.4\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0J9D6tG1V1Z"
      },
      "source": [
        "# a simple rnn model for tagging a sentence with POS tags!\n",
        "\n",
        "class RNNPOStagger(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        # define the layers used in the network.\n",
        "        self.embeddinglayer = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "        self.outputlayer = nn.Linear(hidden_dim*2, tagset_size)\n",
        "         \n",
        "         \n",
        "    def forward(self, text):\n",
        "        # input: a tensor of dimension [{longest sentence in batch} x batch size] \n",
        "        # each sentence in a batch is encoded as a vector (1-d tensor) of word indices corresponding to words in a vocabulary.\n",
        "        \n",
        "        \n",
        "        # converts this input tensor into a size [{longest sentence in batch} x batch size x embedding_dim] tensor. \n",
        "        # each word index is replaced by a length [embedding_dim] vector that is learned by the model to be an embedding for that word.\n",
        "        text = self.embeddinglayer(text)\n",
        "        \n",
        "        \n",
        "        # this tensor is mapped to a size [{longest sentence} x batch size x hidden_dim*2] tensor.\n",
        "        # \n",
        "        text, _ = self.lstm(text)\n",
        "        \n",
        "        \n",
        "\n",
        "        # finally, this tensor is transformed into a size [{longest sentence} x batch size x tagset_size] tensor.\n",
        "        # we can retrieve which tag the model predicts for a certain word by choosing the index of the highest value in this last dimension\n",
        "        text = self.outputlayer(text)\n",
        "        \n",
        "        \n",
        "        return text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kTSr-WQ29R9"
      },
      "source": [
        "# define fields we use in the dataset.\n",
        "TEXT = data.Field(lower=True)\n",
        "UDPOS = data.Field(unk_token=None)\n",
        "\n",
        "fields = ((\"text\", TEXT), (\"upos\", UDPOS))\n",
        "\n",
        "#load Universal Dependencies dataset from torchtext module as a pytorch dataset object.\n",
        "train_set, dev_set, test_set = datasets.UDPOS.splits(fields, root=\"/content/drive/MyDrive/Colab Notebooks/\", train=\"en-ud-tag.v2.train.txt\", validation=\"en-ud-tag.v2.dev.txt\", test=\"en-ud-tag.v2.test.txt\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHtRvTQTTuIf",
        "outputId": "d789db8a-848d-4a03-f38e-060a8a68a901"
      },
      "source": [
        "# build vocabularies for all fields\n",
        "# vocabularies are a dictionary mapping words (or POS tags) to an index.\n",
        "\n",
        "# word vocabulary only includes words that appear in the train set at least twice\n",
        "# this reduces the vocabulary size to 8866 from ~17000.\n",
        "# all rare words appearing below this threshold are treated as 'unknown'\n",
        "TEXT.build_vocab(train_set, min_freq=2)\n",
        "print(\"Vocabulary size:\")\n",
        "print(len(TEXT.vocab))\n",
        "\n",
        "# build vocabulary consisting of possible tags and assigning a numerical index to them\n",
        "UDPOS.build_vocab(train_set)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:\n",
            "8866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bILVtfo-UEls"
      },
      "source": [
        "# to improve training speed of the model we clump the data into smaller groups called batches, \n",
        "# processing model output of a batch at once and adjusting the parameters after each batch.\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# all 3 sets split into a list of batches. these batches are loaded into GPU memory (if available) for faster computation\n",
        "# sentences of similar length are grouped together: this improves speed because less padding is needed.\n",
        "train_iterator, dev_iterator, test_iterator = data.BucketIterator.splits((train_set, dev_set, test_set), batch_size=BATCH_SIZE, device=device, sort_key=lambda x: len(x.text))\n",
        "\n",
        "\n",
        "# after this step: batches are a tensor of size [{longest sentence in batch}, batch size].\n",
        "# each entry in the tensor is an index corresponding to the word's index in the vocabulary \n",
        "# if a sentence is not as long as the longest in a batch, an index corresponding to a special \"padding\" token is appended to the end of it to fill the tensor."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unw1zPKgUiTv"
      },
      "source": [
        "# define variables corresponding to layer sizes. \n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = len(UDPOS.vocab)\n",
        "\n",
        "# create actual tagger model using specified dimensions.\n",
        "rnn_tagger = RNNPOStagger(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "\n",
        "\n",
        "# define learning rate\n",
        "LR = 0.001\n",
        "# the optimizer is responsible for adjusting the model parameters via gradient descent\n",
        "optimizer = torch.optim.Adam(rnn_tagger.parameters(), lr=LR)\n",
        "# define the loss function we will be using\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTl1EQxIVSlO"
      },
      "source": [
        "# move the model and loss function into GPU memory, if available\n",
        "rnn_tagger = rnn_tagger.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXu4t09wVjeL"
      },
      "source": [
        "# function to train the model for one epoch (one pass through the data)\n",
        "def train(model, criterion, optimizer, iterator):\n",
        "      # enter training mode of pytorch neural network object\n",
        "      model.train()\n",
        "\n",
        "      epoch_loss = 0\n",
        "      epoch_accuracy = 0\n",
        "\n",
        "      # we will adjust the model to minimize loss after each batch in the training set\n",
        "      for batch in iterator:\n",
        "          \n",
        "          # zero out gradient ahead of time\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # apply model to text\n",
        "          outputs = model(batch.text)\n",
        "         \n",
        "          # reshape the tensors to size [(batch size * {longest sent length}), tagset size] and [batch size * {longest sent length}] respectively\n",
        "          outputs = outputs.view(-1, outputs.shape[-1])\n",
        "          batch.upos = torch.flatten(batch.upos)\n",
        "          \n",
        "          # compute loss based on model output\n",
        "          loss = criterion(outputs, batch.upos)\n",
        "          \n",
        "          # compute gradient via backpropagation\n",
        "          loss.backward()\n",
        "\n",
        "          # adjust parameters based on gradient\n",
        "          optimizer.step()\n",
        "\n",
        "          # calculate accuracy`(will be a value between 0 and 1)\n",
        "          \n",
        "          # predicted label = the index with the highest output value along the {tagset size} dimension of outputs tensor   \n",
        "          predicted_labels = torch.argmax(outputs, dim=-1)\n",
        "          \n",
        "          num_correct = 0\n",
        "          for i, label in enumerate(predicted_labels):\n",
        "              \n",
        "              if label.item() == batch.upos[i]:\n",
        "                  num_correct += 1\n",
        "          accuracy = num_correct / len(batch.upos)\n",
        "\n",
        "          epoch_loss += loss.item()\n",
        "          epoch_accuracy += accuracy\n",
        "\n",
        "      avg_loss = epoch_loss / len(iterator)\n",
        "      avg_accuracy = epoch_accuracy / len(iterator)\n",
        "      \n",
        "      return avg_loss, avg_accuracy"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O-MiJK5XdR_"
      },
      "source": [
        "# function to evaluate the model for one epoch. model parameters will not be adjusted\n",
        "def eval(model, criterion, iterator):\n",
        "      # put model into eval mode, ensuring it will not be modified\n",
        "      model.eval()\n",
        "\n",
        "      epoch_loss = 0\n",
        "      epoch_accuracy = 0\n",
        "\n",
        "      # we do not need to compute the gradient during this function. saying not to compute any gradients saves time\n",
        "      with torch.no_grad():\n",
        "          # same loop as in training, but without computing the gradient via loss.backward() and without adjusting parameters with the optimizer.\n",
        "          for batch in iterator:\n",
        "              \n",
        "              outputs = model(batch.text)\n",
        "            \n",
        "              \n",
        "              outputs = outputs.view(-1, outputs.shape[-1])\n",
        "              batch.upos = torch.flatten(batch.upos)\n",
        "\n",
        "              loss = criterion(outputs, batch.upos)\n",
        "              \n",
        "              predicted_labels = torch.argmax(outputs, dim=-1)\n",
        "              num_correct = 0\n",
        "              for i, label in enumerate(predicted_labels):\n",
        "                  \n",
        "                  if label.item() == batch.upos[i]:\n",
        "                      num_correct += 1\n",
        "              accuracy = num_correct / len(batch.upos)\n",
        "\n",
        "              epoch_loss += loss.item()\n",
        "              epoch_accuracy += accuracy\n",
        "          \n",
        "      avg_loss = epoch_loss / len(iterator)\n",
        "      avg_accuracy = epoch_accuracy / len(iterator)\n",
        "      \n",
        "      return avg_loss, avg_accuracy"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyzcoUVC0TLr"
      },
      "source": [
        "# convert raw start and end times into minutes and seconds.\n",
        "def minutes_and_seconds(start, end):\n",
        "    time_elapsed = end - start\n",
        "    \n",
        "    epoch_minutes = int(time_elapsed/60)\n",
        "    epoch_seconds = float(f\"{(time_elapsed - (epoch_minutes*60)):.2f}\")\n",
        "\n",
        "    return epoch_minutes, epoch_seconds"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvnnL7j6YVMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34dc95d8-f959-4f0a-ca1c-71591740ada5"
      },
      "source": [
        "# train for 5 epochs. more can be done, but will take longer\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "best_dev_loss = float('inf')\n",
        "# pass through data once for each epoch\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_accuracy = train(rnn_tagger, criterion, optimizer, train_iterator)\n",
        "    dev_loss, dev_accuracy = eval(rnn_tagger, criterion, dev_iterator)\n",
        "\n",
        "    # if model performs better on dev set, we save this model state and want this state of the model\n",
        "    if dev_loss < best_dev_loss:\n",
        "        best_dev_loss = dev_loss\n",
        "        torch.save(rnn_tagger.state_dict(), 'best_tagger_model.pt')\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_minutes, epoch_seconds = minutes_and_seconds(start_time, end_time)\n",
        "\n",
        "    print(\"Epoch\", epoch)\n",
        "    print(f\"Epoch Time: {epoch_minutes} min {epoch_seconds} s\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} Train Accuracy: {100*train_accuracy:.3f}\")\n",
        "    print(f\"Dev Loss:   {dev_loss:.4f} Dev Accuracy:   {100*dev_accuracy:.3f}\")\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Epoch Time: 0 min 34.35 s\n",
            "Train Loss: 0.4063 Train Accuracy: 89.021\n",
            "Dev Loss:   0.7170 Dev Accuracy:   80.102\n",
            "\n",
            "\n",
            "Epoch 1\n",
            "Epoch Time: 0 min 34.56 s\n",
            "Train Loss: 0.1255 Train Accuracy: 96.193\n",
            "Dev Loss:   0.5184 Dev Accuracy:   84.330\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "Epoch Time: 0 min 34.68 s\n",
            "Train Loss: 0.0801 Train Accuracy: 97.555\n",
            "Dev Loss:   0.4602 Dev Accuracy:   85.028\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "Epoch Time: 0 min 34.75 s\n",
            "Train Loss: 0.0587 Train Accuracy: 98.244\n",
            "Dev Loss:   0.4256 Dev Accuracy:   86.577\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "Epoch Time: 0 min 35.07 s\n",
            "Train Loss: 0.0436 Train Accuracy: 98.717\n",
            "Dev Loss:   0.4208 Dev Accuracy:   86.478\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2DLfI5I1_YU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa2bbff-3252-43bb-e47a-c3482eb9e32f"
      },
      "source": [
        "# load best state from training\n",
        "rnn_tagger.load_state_dict(torch.load('best_tagger_model.pt'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUkTZEZdxCNH",
        "outputId": "d52c2e61-4db2-4188-d7a7-3f27d08faca1"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "dev_loss, dev_accuracy = eval(rnn_tagger, criterion, dev_iterator)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "epoch_minutes, epoch_seconds = minutes_and_seconds(start_time, end_time)\n",
        "\n",
        "\n",
        "print(f\"English dev set accuracy: {dev_accuracy*100:.2f}%\")\n",
        "print(f\"Tagging Time: {epoch_minutes} min {epoch_seconds} s\")\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English dev set accuracy: 86.48%\n",
            "Tagging Time: 0 min 1.23 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsW4NZ1sHVya",
        "outputId": "2eefa8af-f693-4c59-a89f-6325e482c217"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "test_loss, test_accuracy = eval(rnn_tagger, criterion, test_iterator)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "epoch_minutes, epoch_seconds = minutes_and_seconds(start_time, end_time)\n",
        "\n",
        "\n",
        "print(f\"English test set accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"Tagging Time: {epoch_minutes} min {epoch_seconds} s\")\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English test set accuracy: 86.06%\n",
            "Tagging Time: 0 min 1.23 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osbsVDoqC8iL"
      },
      "source": [
        "# Other Languages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0orsnm6-QBWB"
      },
      "source": [
        "# CITATION: code for converting a pandas dataframe to a dataset https://gist.github.com/nissan/ccb0553edb6abafd20c3dec34ee8099d\n",
        "# adjusted fields slightly\n",
        "class DataFrameDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, df, text_field, tag_field, **kwargs):\n",
        "        fields = (('text', text_field), ('upos', tag_field))\n",
        "        examples = []\n",
        "        for i, row in df.iterrows():\n",
        "            label = row.upos\n",
        "            text = row.text\n",
        "            examples.append(data.Example.fromlist([text, label], fields))\n",
        "\n",
        "        super().__init__(examples, fields, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def sort_key(ex):\n",
        "        return len(ex.text)\n",
        "\n",
        "    @classmethod\n",
        "    def splits(cls, text_field, tag_field, train_df, val_df, test_df, **kwargs):\n",
        "        train_data, val_data, test_data = (None, None, None)\n",
        "\n",
        "        if train_df is not None:\n",
        "            train_data = cls(train_df.copy(), text_field, tag_field, **kwargs)\n",
        "        if val_df is not None:\n",
        "            val_data = cls(val_df.copy(), text_field, tag_field, **kwargs)\n",
        "        if test_df is not None:\n",
        "            test_data = cls(test_df.copy(), text_field, tag_field, **kwargs)\n",
        "\n",
        "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmBCor7YhxBr"
      },
      "source": [
        "# converts a conllu file to a pandas dataframe\n",
        "def conllu_to_df(inputfile):\n",
        "    data = []\n",
        "\n",
        "    input = pyconll.iter_from_file(inputfile)\n",
        "    sent_words = []\n",
        "    for sentence in input:\n",
        "        sent_words = []\n",
        "        sent_tags = []\n",
        "        for word in sentence:\n",
        "            if word.upos != None: \n",
        "                # conllu files include some irregularities, \n",
        "                # such as words that are the combination of 2 words before the tokenization splits them, which have no POS tag. \n",
        "                # these are not considered but their component words and POS are.\n",
        "                sent_words.append(word.form)\n",
        "                sent_tags.append(word.upos)\n",
        "        data.append([sent_words, sent_tags])\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(data, columns = ['text', 'upos'])\n",
        "    return df"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arb8sKANDDpO"
      },
      "source": [
        "# function that performs all required steps on a non-english language. \n",
        "# returns the model, so that the model can be used to tag sentences as input by user.\n",
        "def test_language(path, language, epochs):\n",
        "    train_df_lang = conllu_to_df(path+'-train.conllu')\n",
        "    dev_df_lang = conllu_to_df(path+'-dev.conllu')\n",
        "    test_df_lang = conllu_to_df(path+'-test.conllu')\n",
        "\n",
        "    TEXTlang = data.Field(lower=True)\n",
        "    UDPOSlang = data.Field()\n",
        "\n",
        "    train_set_lang, dev_set_lang, test_set_lang = DataFrameDataset.splits(TEXTlang, UDPOSlang, train_df_lang, dev_df_lang, test_df_lang)\n",
        "\n",
        "    TEXTlang.build_vocab(train_set_lang, min_freq=2)\n",
        "    print(language, \"vocabulary size:\")\n",
        "    print(len(TEXTlang.vocab))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    UDPOSlang.build_vocab(train_set_lang)\n",
        "\n",
        "    train_iterator_lang, dev_iterator_lang, test_iterator_lang = data.BucketIterator.splits((train_set_lang, dev_set_lang, test_set_lang), batch_size=BATCH_SIZE, device=device, sort_key=lambda x: len(x.text))\n",
        "\n",
        "    rnn_tagger_lang = RNNPOStagger(len(TEXTlang.vocab), EMBEDDING_DIM, HIDDEN_DIM, len(UDPOSlang.vocab))\n",
        "\n",
        "    optimizer_lang = torch.optim.Adam(rnn_tagger_lang.parameters(), lr=LR)\n",
        "\n",
        "    rnn_tagger_lang = rnn_tagger_lang.to(device)\n",
        "\n",
        "    \n",
        "\n",
        "    best_dev_loss = float('inf')\n",
        "    # pass through data once for each epoch\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_accuracy = train(rnn_tagger_lang, criterion, optimizer_lang, train_iterator_lang)\n",
        "        dev_loss, dev_accuracy = eval(rnn_tagger_lang, criterion, dev_iterator_lang)\n",
        "\n",
        "        # if model performs better on dev set, we save this model state and want this state of the model\n",
        "        if dev_loss < best_dev_loss:\n",
        "            best_dev_loss = dev_loss\n",
        "            torch.save(rnn_tagger_lang.state_dict(), 'best_'+language.lower()+'_tagger_model.pt')\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_minutes, epoch_seconds = minutes_and_seconds(start_time, end_time)\n",
        "\n",
        "        print(\"Epoch\", epoch)\n",
        "        print(f\"Epoch Time: {epoch_minutes} min {epoch_seconds} s\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} Train Accuracy: {100*train_accuracy:.3f}\")\n",
        "        print(f\"Dev Loss:   {dev_loss:.4f} Dev Accuracy:   {100*dev_accuracy:.3f}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "    rnn_tagger_lang.load_state_dict(torch.load('best_'+language.lower()+'_tagger_model.pt'))\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    dev_loss, dev_accuracy = eval(rnn_tagger_lang, criterion, dev_iterator_lang)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_minutes, epoch_seconds = minutes_and_seconds(start_time, end_time)\n",
        "\n",
        "\n",
        "    print(f\"Dev set accuracy: {dev_accuracy*100:.2f}%\")\n",
        "    print(f\"Tagging Time: {epoch_minutes} min {epoch_seconds} s\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    test_loss, test_accuracy = eval(rnn_tagger_lang, criterion, test_iterator_lang)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_minutes, epoch_seconds = minutes_and_seconds(start_time, end_time)\n",
        "\n",
        "\n",
        "    print(f\"Test set accuracy: {test_accuracy*100:.2f}%\")\n",
        "    print(f\"Tagging Time: {epoch_minutes} min {epoch_seconds} s\")\n",
        "\n",
        "\n",
        "    return rnn_tagger_lang"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF4A51INwps-"
      },
      "source": [
        "# German"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoD97GkxDGNh",
        "outputId": "ad2e1cff-c04e-4e2b-9c0d-938b8e18fa4c"
      },
      "source": [
        "rnn_tagger_german = test_language('/content/drive/MyDrive/Colab Notebooks/de_gsd-ud', 'German', 5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "German vocabulary size:\n",
            "14710\n",
            "\n",
            "\n",
            "Epoch 0\n",
            "Epoch Time: 0 min 35.54 s\n",
            "Train Loss: 0.4503 Train Accuracy: 86.716\n",
            "Dev Loss:   0.6558 Dev Accuracy:   79.371\n",
            "\n",
            "\n",
            "Epoch 1\n",
            "Epoch Time: 0 min 35.47 s\n",
            "Train Loss: 0.1699 Train Accuracy: 94.341\n",
            "Dev Loss:   0.4335 Dev Accuracy:   86.204\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "Epoch Time: 0 min 35.14 s\n",
            "Train Loss: 0.1185 Train Accuracy: 96.102\n",
            "Dev Loss:   0.3717 Dev Accuracy:   88.293\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "Epoch Time: 0 min 35.16 s\n",
            "Train Loss: 0.0908 Train Accuracy: 97.064\n",
            "Dev Loss:   0.3373 Dev Accuracy:   89.259\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "Epoch Time: 0 min 34.85 s\n",
            "Train Loss: 0.0720 Train Accuracy: 97.721\n",
            "Dev Loss:   0.3096 Dev Accuracy:   90.364\n",
            "\n",
            "\n",
            "Dev set accuracy: 90.36%\n",
            "Tagging Time: 0 min 0.61 s\n",
            "Test set accuracy: 89.14%\n",
            "Tagging Time: 0 min 0.82 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YgQirT3wjzv"
      },
      "source": [
        "# Spanish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_h9Jv-uDZqz",
        "outputId": "3f33a46f-4d44-472e-b59b-95fc03412ac1"
      },
      "source": [
        "rnn_tagger_spanish = test_language('/content/drive/MyDrive/Colab Notebooks/es_gsd-ud', 'Spanish', 5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spanish vocabulary size:\n",
            "17588\n",
            "\n",
            "\n",
            "Epoch 0\n",
            "Epoch Time: 0 min 53.11 s\n",
            "Train Loss: 0.4036 Train Accuracy: 89.093\n",
            "Dev Loss:   0.4397 Dev Accuracy:   86.523\n",
            "\n",
            "\n",
            "Epoch 1\n",
            "Epoch Time: 0 min 53.62 s\n",
            "Train Loss: 0.1315 Train Accuracy: 95.865\n",
            "Dev Loss:   0.2887 Dev Accuracy:   90.735\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "Epoch Time: 0 min 53.03 s\n",
            "Train Loss: 0.0903 Train Accuracy: 97.158\n",
            "Dev Loss:   0.2338 Dev Accuracy:   92.352\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "Epoch Time: 0 min 53.7 s\n",
            "Train Loss: 0.0686 Train Accuracy: 97.821\n",
            "Dev Loss:   0.2104 Dev Accuracy:   93.094\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "Epoch Time: 0 min 53.92 s\n",
            "Train Loss: 0.0553 Train Accuracy: 98.247\n",
            "Dev Loss:   0.1994 Dev Accuracy:   93.340\n",
            "\n",
            "\n",
            "Dev set accuracy: 93.34%\n",
            "Tagging Time: 0 min 1.87 s\n",
            "Test set accuracy: 93.95%\n",
            "Tagging Time: 0 min 0.72 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKTjey02yvGo"
      },
      "source": [
        "# Dutch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wktCZ6xxDo5b",
        "outputId": "9b69bda1-9b7d-443a-a3c6-27ae38936dfd"
      },
      "source": [
        "rnn_tagger_dutch = test_language('/content/drive/MyDrive/Colab Notebooks/nl_alpino-ud', 'Dutch', 5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dutch vocabulary size:\n",
            "9630\n",
            "\n",
            "\n",
            "Epoch 0\n",
            "Epoch Time: 0 min 27.55 s\n",
            "Train Loss: 0.4207 Train Accuracy: 88.312\n",
            "Dev Loss:   0.6177 Dev Accuracy:   80.495\n",
            "\n",
            "\n",
            "Epoch 1\n",
            "Epoch Time: 0 min 27.2 s\n",
            "Train Loss: 0.1462 Train Accuracy: 95.426\n",
            "Dev Loss:   0.4348 Dev Accuracy:   86.405\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "Epoch Time: 0 min 27.3 s\n",
            "Train Loss: 0.0965 Train Accuracy: 97.001\n",
            "Dev Loss:   0.3667 Dev Accuracy:   87.839\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "Epoch Time: 0 min 27.87 s\n",
            "Train Loss: 0.0706 Train Accuracy: 97.803\n",
            "Dev Loss:   0.3169 Dev Accuracy:   89.378\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "Epoch Time: 0 min 27.56 s\n",
            "Train Loss: 0.0555 Train Accuracy: 98.290\n",
            "Dev Loss:   0.2956 Dev Accuracy:   90.252\n",
            "\n",
            "\n",
            "Dev set accuracy: 90.25%\n",
            "Tagging Time: 0 min 0.56 s\n",
            "Test set accuracy: 89.75%\n",
            "Tagging Time: 0 min 0.57 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cvj8kvF0dfC"
      },
      "source": [
        "# Indonesian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s1wvRLJ8rIS",
        "outputId": "bd426a63-d9de-4820-e479-25c3f10fa620"
      },
      "source": [
        "rnn_tagger_indonesian = test_language('/content/drive/MyDrive/Colab Notebooks/id_gsd-ud', 'Indonesian', 5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indonesian vocabulary size:\n",
            "6737\n",
            "\n",
            "\n",
            "Epoch 0\n",
            "Epoch Time: 0 min 17.5 s\n",
            "Train Loss: 0.6422 Train Accuracy: 82.551\n",
            "Dev Loss:   1.0511 Dev Accuracy:   67.859\n",
            "\n",
            "\n",
            "Epoch 1\n",
            "Epoch Time: 0 min 17.64 s\n",
            "Train Loss: 0.2831 Train Accuracy: 91.054\n",
            "Dev Loss:   0.6777 Dev Accuracy:   78.611\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "Epoch Time: 0 min 17.85 s\n",
            "Train Loss: 0.1904 Train Accuracy: 94.084\n",
            "Dev Loss:   0.5108 Dev Accuracy:   83.985\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "Epoch Time: 0 min 17.21 s\n",
            "Train Loss: 0.1450 Train Accuracy: 95.483\n",
            "Dev Loss:   0.4350 Dev Accuracy:   85.934\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "Epoch Time: 0 min 17.79 s\n",
            "Train Loss: 0.1122 Train Accuracy: 96.540\n",
            "Dev Loss:   0.4005 Dev Accuracy:   87.162\n",
            "\n",
            "\n",
            "Dev set accuracy: 87.16%\n",
            "Tagging Time: 0 min 0.8 s\n",
            "Test set accuracy: 88.36%\n",
            "Tagging Time: 0 min 0.77 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiXk2caQ2OuD"
      },
      "source": [
        "# Korean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ2mMRPs37mC",
        "outputId": "09e21db3-73a2-45a1-fd86-ec0ec4e39b6d"
      },
      "source": [
        "rnn_tagger_korean = test_language('/content/drive/MyDrive/Colab Notebooks/ko_kaist-ud', 'Korean', 5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Korean vocabulary size:\n",
            "25981\n",
            "\n",
            "\n",
            "Epoch 0\n",
            "Epoch Time: 0 min 28.97 s\n",
            "Train Loss: 0.6579 Train Accuracy: 79.214\n",
            "Dev Loss:   0.8282 Dev Accuracy:   73.670\n",
            "\n",
            "\n",
            "Epoch 1\n",
            "Epoch Time: 0 min 28.91 s\n",
            "Train Loss: 0.3543 Train Accuracy: 88.546\n",
            "Dev Loss:   0.6734 Dev Accuracy:   76.605\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "Epoch Time: 0 min 29.07 s\n",
            "Train Loss: 0.2556 Train Accuracy: 91.676\n",
            "Dev Loss:   0.5948 Dev Accuracy:   79.907\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "Epoch Time: 0 min 29.07 s\n",
            "Train Loss: 0.1995 Train Accuracy: 93.446\n",
            "Dev Loss:   0.5547 Dev Accuracy:   81.107\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "Epoch Time: 0 min 28.74 s\n",
            "Train Loss: 0.1632 Train Accuracy: 94.579\n",
            "Dev Loss:   0.5654 Dev Accuracy:   80.199\n",
            "\n",
            "\n",
            "Dev set accuracy: 81.11%\n",
            "Tagging Time: 0 min 1.14 s\n",
            "Test set accuracy: 80.52%\n",
            "Tagging Time: 0 min 1.33 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el2D0bo_Dz90"
      },
      "source": [
        "# Chinese"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29fdSn2bDy6_",
        "outputId": "af3275e7-4b85-4b37-d018-f66d6cfb85ac"
      },
      "source": [
        "rnn_tagger_chinese = test_language('/content/drive/MyDrive/Colab Notebooks/zh_gsdsimp-ud', 'Chinese', 5)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chinese vocabulary size:\n",
            "7094\n",
            "\n",
            "\n",
            "Epoch 0\n",
            "Epoch Time: 0 min 12.86 s\n",
            "Train Loss: 0.8118 Train Accuracy: 76.668\n",
            "Dev Loss:   1.1859 Dev Accuracy:   61.576\n",
            "\n",
            "\n",
            "Epoch 1\n",
            "Epoch Time: 0 min 12.5 s\n",
            "Train Loss: 0.4236 Train Accuracy: 86.514\n",
            "Dev Loss:   0.7509 Dev Accuracy:   76.200\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "Epoch Time: 0 min 12.67 s\n",
            "Train Loss: 0.2810 Train Accuracy: 91.187\n",
            "Dev Loss:   0.5652 Dev Accuracy:   81.674\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "Epoch Time: 0 min 12.77 s\n",
            "Train Loss: 0.2083 Train Accuracy: 93.470\n",
            "Dev Loss:   0.4696 Dev Accuracy:   84.697\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "Epoch Time: 0 min 12.69 s\n",
            "Train Loss: 0.1682 Train Accuracy: 94.749\n",
            "Dev Loss:   0.4184 Dev Accuracy:   86.497\n",
            "\n",
            "\n",
            "Dev set accuracy: 86.50%\n",
            "Tagging Time: 0 min 0.66 s\n",
            "Test set accuracy: 87.51%\n",
            "Tagging Time: 0 min 0.68 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjwlMPMccD7y"
      },
      "source": [
        "# Testing on user input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sbNlsZwcDIf"
      },
      "source": [
        "# takes as input a sentence as a string and returns a list of (token, predicted tag) tuples\n",
        "# tokenizes only by whitespace\n",
        "def tag_sent(text, model):\n",
        "    # turn input into a 1-d tensor containing word indices\n",
        "    token_indices = []\n",
        "    for token in text.split():\n",
        "        token_indices.append(TEXT.vocab.stoi[token.lower()])\n",
        "    token_indices = torch.unsqueeze(torch.LongTensor(token_indices).to(device), -1)\n",
        "    \n",
        "\n",
        "    # predict outputs, same way as done in train/eval functions\n",
        "    outputs = model(token_indices)\n",
        "    \n",
        "    outputs = outputs.view(-1, outputs.shape[-1])\n",
        "    predicted_labels = torch.argmax(outputs, dim=-1)\n",
        "    predicted_labels = predicted_labels.tolist()\n",
        "    \n",
        "    labeled_sent = []\n",
        "    \n",
        "    for i, word in enumerate(text.split()):\n",
        "        labeled_sent.append((word, UDPOS.vocab.itos[predicted_labels[i]]))\n",
        "    \n",
        "       \n",
        "    return labeled_sent"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64l-pPY1cICV",
        "outputId": "7bd6974d-5cf4-45da-b06f-f4a6c3a65c4a"
      },
      "source": [
        "sent = \"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\"\n",
        "\n",
        "labeled_sent = tag_sent(sent, rnn_tagger)\n",
        "print(labeled_sent)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Buffalo', 'PROPN'), ('buffalo', 'PROPN'), ('Buffalo', 'PROPN'), ('buffalo', 'PROPN'), ('buffalo', 'NUM'), ('buffalo', 'NUM'), ('Buffalo', 'NUM'), ('buffalo', 'NUM')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1D48f1xOCSu"
      },
      "source": [
        "Feel free to test this yourself with any strings and languages desired! "
      ]
    }
  ]
}